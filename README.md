Customer Behavior Analysis with PySparkThis project performs a large-scale analysis of e-commerce customer data to uncover purchasing patterns, segment customers, and identify behavior trends using Apache Spark's distributed computing capabilities via PySpark.The entire analysis pipeline—from data generation to machine learning and insight reporting—is executed in a distributed manner, making it suitable for big data workloads.Project PipelineData Generation: A scalable function generates a large synthetic dataset of customers and transactions directly within the Spark cluster, avoiding memory bottlenecks on the driver node.Feature Engineering: Raw transaction data is enriched to create key metrics like Recency, Frequency, and Monetary (RFM) values, along with customer lifetime metrics.Customer Segmentation (RFM Analysis): Customers are segmented into distinct groups like "Champions," "Loyal Customers," and "At Risk" based on their RFM scores.Machine Learning Clustering: K-Means clustering is applied to group customers into distinct personas based on their demographic and behavioral features.Behavioral Pattern Analysis: The analysis explores purchasing patterns related to the day of the week, product categories, and payment methods.Insights and Recommendations: The final step summarizes key business metrics and provides actionable recommendations based on the analysis.Technologies UsedApache Spark & PySpark: For distributed data processing and analytics.PySpark SQL: For data manipulation and feature engineering.PySpark MLlib: For K-Means clustering.Python: For scripting and data generation logic.Results & Key InsightsThe analysis provides a comprehensive overview of the customer base. Below is a sample of the final output from the script, showing customer segmentation and churn risk.Key findings from the analysis include:Customer Segmentation: Identification of high-value "Champions" and "Loyal Customers" who are crucial for business revenue.Churn Risk: A significant number of customers were identified as "At Risk," allowing for targeted retention campaigns.Behavioral Patterns: Clear trends in weekend vs. weekday purchasing and popularity of certain product categories.Running the ProjectPrerequisitesJava 8/11Apache Spark 3.xPython 3.7+1. Clone the Repositorygit clone [https://github.com/your-username/customer-behavior-analysis-pyspark.git](https://github.com/your-username/customer-behavior-analysis-pyspark.git)
cd customer-behavior-analysis-pyspark
2. Install DependenciesIt's recommended to use a virtual environment.python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
3. Run the PySpark ScriptUse spark-submit to run the analysis script. Adjust the driver memory or executor cores as needed for your environment.spark-submit customer_analysis.py
Spark Performance & MonitoringThe script is designed to run efficiently on a distributed cluster. The Spark UI provides insights into job execution, task distribution, and resource utilization.Spark Jobs UIThe Jobs tab shows the high-level Spark jobs triggered by the PySpark script. Each job corresponds to an action in the code.Stages UIEach job is broken down into stages. This view is useful for identifying performance bottlenecks in the execution plan.Executors UIThis view shows the distribution of tasks and storage across the cluster's worker nodes (executors).This project was created to demonstrate end-to-end data analysis and engineering using distributed computing with PySpark.
